{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 12.3 - Coding\n",
    "\n",
    "This is the coding portion of the homework assignment for Section 12.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "import numpy as np\n",
    "from jax import grad\n",
    "from jax import numpy as jnp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "Newton"
    ]
   },
   "outputs": [],
   "source": [
    "def newton_min(\n",
    "    f: Callable[[float], float],\n",
    "    x0: float,\n",
    "    eps: float = 1e-2,\n",
    "    maxiter: int = 30\n",
    ") -> float:\n",
    "    \"\"\"Approximates the minimizer of a callable scalar function\n",
    "    f: R -> R using Newton's method for finding critical points:\n",
    "    \n",
    "    x_{k+1} = x_{k} - (f'(x_{k})/f''(x_{k}))\n",
    "\n",
    "    This method assumes convergence when \n",
    "    |x_{k+1} - x_{k}| < eps \n",
    "\n",
    "    Raises a RuntimeError if the algorithm does not converge\n",
    "    within a given number of iterations.\n",
    "\n",
    "    Args:\n",
    "        f (Callable[[float], float]): A callable function f:R -> R \n",
    "            which we would like to minimize \n",
    "        x0 (float): An initial guess for a minimizer \n",
    "        eps (float): An error tolerance for successive iterations\n",
    "            to determine convergence:\n",
    "            |x_{k+1} - x_{k}| < eps \n",
    "            Defaults to 0.01\n",
    "        maxiter (int): The maximum number of iterations the algorithm\n",
    "            should be allowed to run before throwing an error \n",
    "            indicating a lack of convergence. \n",
    "            Defaults to 30\n",
    "    \n",
    "    Returns:\n",
    "        float: A close approximation to a critical point of f\n",
    "    \"\"\"\n",
    "    # Get f' and f'' as callable functions\n",
    "    df = grad(f)\n",
    "    d2f = grad(df)\n",
    "\n",
    "    # Newton iteration for finding critical points \n",
    "    x_k = x0 \n",
    "    for _ in range(maxiter):\n",
    "        x_kplus1 = x_k - (df(x_k)/d2f(x_k))\n",
    "        if abs(x_kplus1 - x_k) < eps:\n",
    "            return x_kplus1 \n",
    "        x_k = x_kplus1\n",
    "    return x_kplus1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12.15 \n",
    "Consider the quadratic function \n",
    "$$f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top Q \\mathbf{x} - \\mathbf{b}^\\top \\mathbf{x} + c$$\n",
    "\n",
    "In the function `quad_grad_descent()`, implement an exact gradient descent method for quadratic functions in the style of $f$, above.\n",
    "\n",
    "Recall that exact gradient descent for quadratic functions such as $f$ has the iteration scheme\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha_k Df(\\mathbf{x}_k)^\\top$$\n",
    "where \n",
    "$$\\alpha_k := \\frac{Df(\\mathbf{x}_k)Df(\\mathbf{x}_k)^\\top}{Df(\\mathbf{x}_k)Q Df(\\mathbf{x}_k)^\\top}$$\n",
    "and \n",
    "$$Df(\\mathbf{x}_k)^\\top = Q\\mathbf{x}_k - \\mathbf{b}$$\n",
    "\n",
    "Your code accept the matrix $Q$ and the vector $\\mathbf{b}$ from the quadratic equation $f$ above, along with an initial guess $\\mathbf{x}_0$ for the minimizer, an error tolerance $\\varepsilon$, and a maximum number of iterations, and should return:\n",
    "\n",
    "1. A close approximation to the local minimizer $\\mathbf{x}^*$ of the quadratic function $f$ given above, and\n",
    "2. The number of iterations used in the algorithm\n",
    "3. A boolean value indicating whether or not your algorithm converged.\n",
    "\n",
    "Your code should adhere to the following guidelines:\n",
    "\n",
    "1. Use the convergence criterion of $\\lVert Df(\\mathbf{x}_k)^\\top \\rVert_{2} < \\varepsilon$ to know when to terminate the algorithm.\n",
    "2. If your method fails to converge within the maximum number of iterations, just return the most recent update as the solution, along with the maximum number of iterations as the number of iterations, and `False` for whether the algorithm converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": [
     "Problem 12.15"
    ]
   },
   "outputs": [],
   "source": [
    "def quad_grad_descent(\n",
    "    Q: np.ndarray,\n",
    "    b: np.ndarray,\n",
    "    x0: np.ndarray,\n",
    "    eps: float = 1e-6,\n",
    "    maxiter: int = 10000\n",
    ") -> tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    A simple implementation of the exact gradient descent\n",
    "    method to find local minimizers of quadratic functions\n",
    "    taking the form\n",
    "    \n",
    "    f(x) = 1/2 x^T Q x - b^T x + c\n",
    "\n",
    "    where Q is a positive definite nxn matrix, and b is a length-n vector.\n",
    "\n",
    "    Uses the convergence criterion\n",
    "    ||Df(x_{k})^T||_2 < eps\n",
    "\n",
    "    If the algorithm fails to converge within the given number of iteraions,\n",
    "    raises a RuntimeError.\n",
    "    \n",
    "    Parameters: \n",
    "        Q ((n,n) ndarray) - A nxn positive definite matrix used in \n",
    "            defining the quadratic function f\n",
    "        b ((n,) ndarray) - A length n array used in definining\n",
    "            the quadratic function f\n",
    "        x0 ((n,) ndarray) - An initial guess (a length-n array)\n",
    "        eps (float) - A small number representing the error tolerance\n",
    "            in the 2-norm of the gradient; \n",
    "            that is, a number indicating convergence when\n",
    "            ||Df(x_{k})^T||_2 < eps\n",
    "        maxiter (int) - The maximum number of iterations the algorithm\n",
    "            is allowed to go before raising a RuntimeError indicating\n",
    "            a lack of convergence\n",
    "\n",
    "    Returns: \n",
    "        (n,) ndarray - A close approximation of the local minimizer x* of \n",
    "            f(x) = 1/2 x^T Q x - b^T x + c\n",
    "        int - The number of iterations it took to converge\n",
    "        bool - Whether or not the algorithm converged to within the\n",
    "            error tolerance\n",
    "    \"\"\"\n",
    "    x = x0 \n",
    "    \n",
    "    for i in range(maxiter + 1):\n",
    "        Df_x = (Q @ x - b).T\n",
    "\n",
    "        if np.linalg.norm(Df_x.T, 2) < eps:\n",
    "            return x, i, True\n",
    "\n",
    "        alpha = np.dot(Df_x, Df_x) / (Df_x @ Q @ Df_x.T)\n",
    "        x = x - alpha*Df_x \n",
    "\n",
    "    return x, maxiter, False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following cell to approximate the minimizer of the function \n",
    "$$f(\\mathbf{x}) = \\begin{bmatrix} x_1 & x_2 & x_3 \\end{bmatrix} \\begin{bmatrix}2 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 2  \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} - \\begin{bmatrix}2 & 4 & -3 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$$\n",
    "which should be $(2.75, 3.5, 0.25)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate Minimizer:  [2.75000052 3.50000096 0.25000052]\n",
      "Number of Iterations of Algorithm:  40\n",
      "Converged to Within Tolerance:  True\n"
     ]
    }
   ],
   "source": [
    "# ------------------- DO NOT EDIT. JUST RUN. ------------------- #\n",
    "Q = np.array([[2,-1,0],[-1,2,-1],[0,-1,2]])\n",
    "b = np.array([2,4,-3])\n",
    "x0 = np.array([1,6,3])  # Initial guess for minimizer\n",
    "min_approx, num_iters, converged = quad_grad_descent(Q, b, x0)\n",
    "print(\"Approximate Minimizer: \", min_approx)\n",
    "print(\"Number of Iterations of Algorithm: \", num_iters)\n",
    "print(\"Converged to Within Tolerance: \", converged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12.16\n",
    "\n",
    "Now, in the function `exact_grad_descent()`, write up an implementation of exact gradient descent for an arbitrary function $f: \\mathbb{R}^n \\to \\mathbb{R}$. \n",
    "\n",
    "Recall that exact gradient descent for an arbitrary function $f: \\mathbb{R}^n \\to \\mathbb{R}$ has the iteration scheme\n",
    "$$\\mathbf{x}_{k+1} = \\mathbf{x}_{k} - \\alpha_k Df(\\mathbf{x}_k)^\\top$$\n",
    "where \n",
    "$$\\alpha_k := \\operatorname{argmin}_{\\alpha > 0} \\phi_k(\\alpha)$$\n",
    "and\n",
    "$$\\phi_k(\\alpha) := f(\\mathbf{x}_k - \\alpha Df(\\mathbf{x}_k)^\\top)$$\n",
    "\n",
    "Your implementation should accept a callable function $f: \\mathbb{R}^n \\to \\mathbb{R}$, an initial guess $\\mathbf{x}_0$ for the minimizer, an error tolerance $\\varepsilon$, and a maximum number of iterations.\n",
    "\n",
    "Your code should return:\n",
    "\n",
    "1. A close approximation to the local minimizer $\\mathbf{x}^*$ of the quadratic function $f$ given above, and\n",
    "2. The number of iterations used in the algorithm\n",
    "3. A boolean value indicating whether or not your algorithm converged.\n",
    "\n",
    "Your code should adhere to these guidelines:\n",
    "\n",
    "1. Assume that all passed-in functions $f$ are coded using `jax`'s version of numpy `jnp` in order to allow for automatic differentiation\n",
    "2. Before any iteration has happened, use the `jax` function `grad()` to get a callable version of the derivative $Df(\\mathbf{x})$ of the provided function $f$.\n",
    "3. At each update step, use the provided code `newton_min()` (found at the top of this file) to find $\\alpha_k$ by minimizing the function $\\phi_k(\\alpha)$ (HINT: You may need to make a new lambda function at each iteration to pass to `newton_min()`)\n",
    "4. Use the criterion $\\lVert Df(\\mathbf{x}_{k})^\\top \\rVert_{2} < \\varepsilon$ to determine convergence.\n",
    "5. If your method fails to converge within the maximum number of iterations, just return the most recent update as the solution, along with the maximum number of iterations as the number of iterations, and `False` for whether the algorithm converged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEFORE DOING THIS, HOWEVER...**\n",
    "\n",
    "Note that `newton_min()` takes in an initial guess for your parameter $\\alpha$ during the \"line search\" part of the algorithm in step (3). You must choose a good initial guess to guarantee convergence. Which $\\alpha_0$ guess will you choose and why?\n",
    "\n",
    "(HINT: It should be very, very small to be a good initial guess. Can you think about why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESPONSE**: _use something small like 10^-6.  This is because the true alpha we're looking for is likely extremely small already, and we want to approach it slowly and directly as opposed to large and zigzaggy_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, code up the algorithm as described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def exact_grad_descent(\n",
    "    f: Callable[[jnp.ndarray], float],\n",
    "    x0: jnp.ndarray,\n",
    "    eps: float = 1e-6,\n",
    "    maxiter: int = 1000\n",
    ") -> tuple[jnp.ndarray, int]:\n",
    "    \"\"\"\n",
    "    An implementation of the exact gradient descent method for any\n",
    "    arbitrary function f. Estimates a local minimizer of f close to x0.\n",
    "\n",
    "    Uses the convergence criterion\n",
    "    ||Df(x_{k})^T||_2 < eps\n",
    "\n",
    "    If the algorithm fails to converge within the given number of iteraions,\n",
    "    raises a RuntimeError.\n",
    "    \n",
    "    Args: \n",
    "        f (Callable[[jnp.ndarray], float]) - An arbitrary\n",
    "            function to minimize, from R^n to R.\n",
    "        x0 ((n,) jnp.ndarray) - A length-n jax-compatible\n",
    "            numpy array representing the initial guess/seed\n",
    "            for the gradient descent algorithm\n",
    "        eps (float) - A small number representing the error tolerance\n",
    "            in the 2-norm of the gradient; \n",
    "            that is, a number indicating convergence when\n",
    "            ||Df(x_{k})^T||_2 < eps\n",
    "        maxiter (int) - The maximum number of iterations the algorithm\n",
    "            is allowed to go before raising a RuntimeError indicating\n",
    "            a lack of convergence\n",
    "    \n",
    "    Returns: \n",
    "        (n,) jnp.ndarray - A close approximation of the local \n",
    "            minimizer x* of f \n",
    "        int - The number of iterations used in the algorithm\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError - If the algorithm does not converge in the given\n",
    "            number of iterations\n",
    "    \"\"\"\n",
    "\n",
    "    Df = grad(f)\n",
    "\n",
    "    x = x0 \n",
    "    \n",
    "    for i in range(maxiter + 1):\n",
    "        Df_x = Df(x)\n",
    "\n",
    "        if np.linalg.norm(Df_x.T, 2) < eps:\n",
    "            return x, i, True\n",
    "\n",
    "        g = lambda alpha: f(x - alpha * Df_x)\n",
    "        alpha = newton_min(g, 10e-6, eps)\n",
    "        x = x - alpha*Df_x \n",
    "\n",
    "    raise ValueError(f\"Did not converge after {maxiter} iterations\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the following code cell to test the `exact_grad_descent()` function by trying to find the minimizer of\n",
    "$$f(x,y) = -\\cos(x)\\cos(y)e^{-(x-\\pi)^2 + (y-\\pi)^2}$$\n",
    "using the initial guess $(x_0, y_0) = (2.8, 2.8)$.\n",
    "\n",
    "The result should be $(x^*, y^*) = (\\pi, \\pi)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimate for Local Minimizer:  [3.1415927 3.1415927]\n",
      "Number of Iterations:  1\n",
      "Algorithm Converged?  True\n"
     ]
    }
   ],
   "source": [
    "# ------------------- DO NOT EDIT. JUST RUN. ------------------- #\n",
    "x0 = jnp.array([2.9, 2.9])\n",
    "f = lambda x: -jnp.cos(x[0]) * jnp.cos(x[1]) * jnp.exp(-((x[0] - jnp.pi)**2 + (x[1] - jnp.pi)**2))\n",
    "estimate, num_iters, converged = exact_grad_descent(f,x0)\n",
    "print(\"Estimate for Local Minimizer: \", estimate)\n",
    "print(\"Number of Iterations: \", num_iters)\n",
    "print(\"Algorithm Converged? \", converged) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 12.17\n",
    "\n",
    "Apply your `exact_grad_descent()` function from the previous problem to the Rosenbrock function\n",
    "$$f(x,y) = 100(y-x^2)^2 + (1-x)^2$$\n",
    "with an initial guess of $(x_0, y_0) = (-2,2).$ \n",
    "\n",
    "The true minimizer is $(1,1)$. \n",
    "\n",
    "Try to see if you can get it to converge to within $10^{-5}$ of the true minimizer.\n",
    "\n",
    "(Hint: you probably won't without an obscene number of iterations or an insanely small error tolerance - try it to see!)\n",
    "\n",
    "(This cell may take a while to run for high numbers of iterations/small errors. This is normal and okay - so don't sweat it if you can't make it run for high numbers of iterations quickly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Did not converge after 500 iterations",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m eps = \u001b[32m1e-3\u001b[39m             \u001b[38;5;66;03m# Play around with this parameter\u001b[39;00m\n\u001b[32m      5\u001b[39m maxiter = \u001b[32m500\u001b[39m           \u001b[38;5;66;03m# Play around with this parameter, too\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mexact_grad_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrosenbrock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mexact_grad_descent\u001b[39m\u001b[34m(f, x0, eps, maxiter)\u001b[39m\n\u001b[32m     52\u001b[39m     alpha = newton_min(g, \u001b[32m10e-6\u001b[39m, eps)\n\u001b[32m     53\u001b[39m     x = x - alpha*Df_x \n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDid not converge after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmaxiter\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m iterations\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Did not converge after 500 iterations"
     ]
    }
   ],
   "source": [
    "# TODO: Edit parameters eps and maxiter in this cell and see what happens.\n",
    "rosenbrock = lambda x: 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2 \n",
    "x0 = jnp.array([-2.,2.])\n",
    "eps = 1e-3             # Play around with this parameter\n",
    "maxiter = 500           # Play around with this parameter, too\n",
    "exact_grad_descent(rosenbrock, x0,eps, maxiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether or not you got it to converge, you'll see that it took a long time to run this algorithm. Why is this? (Hint: Google the Rosenbrock function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESPONSE:** _TODO: The function is designed to be hard to converge.  there's a valley which is easy to find, but the convergence from there to the global minimum is really slow and takes forever._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
